---
title: "[ML][CNN] Chapter 1"
excerpt: "maxout"
date: 2020-04-21
categories:
  - ML
tags:
teaser: /assets/images/teaser/neural_network.jpg
toc: true
toc_label: "Table of contents"
toc_icon: "heart"  # corresponding Font Awesome icon name (without fa prefix)
toc_sticky: true
classes: wide
---

Goal : CNN
---

## maxout

maxout은 dropout의 효과를 극대화시키기 위해 독특한 형태의 활성함수를 고안한 것이라고 볼 수 있습니다. 

![cnn-2](/assets/images/ml/cnn-2.jpg)  

이 수식은 다음과 같이 사용됩니다.  

![cnn-3](/assets/images/ml/cnn-3.jpg)  

일반적으로 hidden layer를 그리면 1개의 layer를 그리기 마렵입니다. 위 그림에서 파란색 원을 무시하고 보면 일반적인 1 : 3 : 1 (in/hidden/out)의 fc구조입니다. 그런데 maxout 구조에서 하나의 hidden layer는 FC 유닛으로 구성된 부분과 max-pooling 유닛으로 구성됩니다.    

전자(녹색)는 전통적인 hidden layer와 달리 활성함수가 없습니다. 단순하게 입력 x를 각각 weight에 곱해서 더하는 형식입니다. 대신에 후자(파란색)의 영역에서 k개의 동일한 위치에 있는 column에서 max값을 취합니다. fc unit에서 활성함수를 적용하는 대신 max-pooling을 적용하는 것입니다. 최족적으로는 입력 unit의 갯수와 같은 출력이 나옵니다.  

한 단씩 뜯어서 보면 다음과 같습니다.  

![cnn-4](/assets/images/ml/cnn-4.jpg)  

위 그림은 1(bias 1) : 3 : 1 maxout unit을 나타냅니다. 이를 통해서 y = x^2를 표현하면 아래와 같이 나타낼 수 있습니다.  

![cnn-5](/assets/images/ml/cnn-5.jpg)  

y = x^2를 3개의 직선으로 근사한 모습입니다. 이 때 직선의 수는 전통적인 hidden layer(활성함수는 적용되지 않지만)인 k입니다. 만약 k가 증가한다면 더 많은 직선을 사용해서 f(x)에 근사할 수 있을 것입니다. 이런 의미해서 maxout은 universal approximator라고 할 수 있습니다.  

# Reference

- [라온피플-maxout](https://m.blog.naver.com/laonple/220836305907)